{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - Master's Degrees from all over!\n",
    "\n",
    "**Knowledge!**\n",
    "\n",
    "Nowadays, society expects everyone, especially you, to have a decent level of knowledge from a relatively wide range of topics. To do so, as you are all doing here, most students choose, after finishing their undergraduate studies, to keep on studying. The natural next step is the Master's Degree!\n",
    "You will be working with a [webpage](https://www.findamasters.com) that provides information and evalutation on many of these master's degrees in many universities worldwide.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# VERY VERY IMPORTANT\n",
    "1. **!!! Read the entire homework before coding anything!!!**\n",
    "2. *My solution is not better than yours, and yours is not better than mine*. In any data analysis task, there **is no** unique way to answer. For this reason, it is crucial (**necessary and mandatory**) that you describe any single decision you take and all your steps.\n",
    "3. Once solving an exercise, comments about the obtained results are **mandatory**. We are not always explicit about where to focus your comments, but we will always want brief sentences about your discoveries.\n",
    "4. We encourage using chatGPT (Bard, Bing, or any other Large Language Models (LLM) chatbot tool) as allies to help you solve your homework, and we were hoping you could learn how to use them properly. However, **using such tools when not explicitly allowed will be considered plagiarism and strictly prohibited**. \n",
    "\n",
    "Now that it is all well settled, let's get on with it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data collection\n",
    "\n",
    "For this homework, there is no provided dataset. Instead, you have to build your own. Your search engine will run on text documents. So, here\n",
    "we detail the procedure to follow for the data collection. We strongly suggest you work on different modules when implementing the required functions. For example, you may have a ```crawler.py``` module, a ```parser.py``` module, and a ```engine.py``` module: this is a good practice that improves readability in reporting and efficiency in deploying the code. Be careful; you are likely dealing with exceptions and other possible issues! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os # To create folder\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1.1. Get the list of master's degree courses\n",
    "\n",
    "We start with the list of courses to include in your corpus of documents. In particular, we focus on web scrapping the [MSc Degrees](https://www.findamasters.com/masters-degrees/msc-degrees/). Next, we want you to **collect the URL** associated with each site in the list from the previously collected list.\n",
    "The list is long and split into many pages. Therefore, we ask you to retrieve only the URLs of the places listed in **the first 400 pages** (each page has 15 courses, so you will end up with 6000 unique master's degree URLs).\n",
    "\n",
    "The output of this step is a `.txt` file whose single line corresponds to the master's URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.findamasters.com/masters-degrees/msc-degrees/'\n",
    "page_urls = [] # list containing all the master's degree URLs\n",
    "\n",
    "# Get the master's degree URL for all 400 pages\n",
    "for page_number in tqdm(range(1, 401), desc=\"Extracting URLs\", unit=\"Page\"):\n",
    "    \n",
    "   # URL of the page \n",
    "    page_url = f\"{url}?PG={page_number}\"\n",
    "\n",
    "    try:\n",
    "        # append to the list the 15 courses of each page\n",
    "        page_urls.extend(crawler.extract_MSc(page_url))\n",
    "\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error in web scraping: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we save the .txt file\n",
    "with open('master_degree_urls.txt','w') as file:\n",
    "    for url in page_urls:\n",
    "        file.write(url + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Crawl master's degree pages\n",
    "\n",
    "Once you get all the URLs in the first 400 pages of the list, you:\n",
    "\n",
    "1. Download the HTML corresponding to each of the collected URLs.\n",
    "2. After you collect a single page, immediately save its `HTML` in a file. In this way, if your program stops for any reason, you will not lose the data collected up to the stopping point.\n",
    "3. Organize the downloaded `HTML` pages into folders. Each folder will contain the `HTML` of the courses on page 1, page 2, ... of the list of master's programs.\n",
    "   \n",
    "__Tip__: Due to the large number of pages you should download, you can use some methods that can help you shorten the time. If you employed a particular process or approach, kindly describe it.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e64fac52ee74546ac8f084e38c4776e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading HTML Files:   0%|          | 0/6001 [00:00<?, ?URL/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here we create 400 folders on our computer and save 15 html files in each one of them\n",
    "\n",
    "# Open the txt file and retrieve each URL\n",
    "with open('/Users/Francesco/Desktop/ADM/HW3/master_degree_urls.txt', 'r') as file:\n",
    "    urls = [line.strip() for line in file]\n",
    "\n",
    "\n",
    "# Initialize variables to save 15 HTML files in each folder\n",
    "i = 0\n",
    "number = 1\n",
    "\n",
    "for url in tqdm(urls, desc=\"Downloading HTML Files\", unit=\"URL\"):\n",
    "    \n",
    "    # To each URL, we need to add the base root of the website\n",
    "    url = 'https://www.findamasters.com' + url\n",
    "\n",
    "    # if I have saved 15 files, change folder\n",
    "    if i == 15:\n",
    "        i = 0\n",
    "        number += 1\n",
    "\n",
    "    folder_path = f'/Users/Francesco/Desktop/ADM/HW3/Page{number}'\n",
    "\n",
    "    # Ensure the folder exists, create it if it doesn't\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # Get the content of the website\n",
    "    r = requests.get(url)\n",
    "    content = r.content\n",
    "\n",
    "    time.sleep(1)  # Add a delay of 1 second between requests to avoid getting banned\n",
    "\n",
    "    # Create a suitable name\n",
    "    url = f'course_{i}'\n",
    "\n",
    "    # Combine the folder_path and file_name to get the full path where to save the file\n",
    "    file_path = os.path.join(folder_path, url)\n",
    "\n",
    "    with open(f'{file_path}.html', 'wb') as html_file:\n",
    "        html_file.write(content)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "    time.sleep(1) # Add a delay of 1 second between requests to avoid getting banned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages\n",
    "\n",
    "At this point, you should have all the HTML documents about the master's degree of interest, and you can start to extract specific information. The list of the information we desire for each course and their format is as follows:\n",
    "\n",
    "1. Course Name (to save as ```courseName```): string;\n",
    "2. University (to save as ```universityName```): string;\n",
    "3. Faculty (to save as ```facultyName```): string\n",
    "4. Full or Part Time (to save as ```isItFullTime```): string;\n",
    "5. Short Description (to save as ```description```): string;\n",
    "6. Start Date (to save as ```startDate```): string;\n",
    "7. Fees (to save as ```fees```): string;\n",
    "8. Modality (to save as ```modality```):string;\n",
    "9. Duration (to save as ```duration```):string;\n",
    "10. City (to save as ```city```): string;\n",
    "11. Country (to save as ```country```): string;\n",
    "12. Presence or online modality (to save as ```administration```): string;\n",
    "13. Link to the page (to save as ```url```): string.\n",
    "    \n",
    "<p align=\"center\">\n",
    "<img src=\"img/example.jpeg\" width = 1000>\n",
    "</p>\n",
    "This are the first rows of the scraped dataset:\n",
    "\n",
    "<div style=\"overflow-x:auto;\">\n",
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th>index</th>\n",
    "    <th>courseName</th>\n",
    "    <th>universityName</th>\n",
    "    <th>facultyName</th>\n",
    "    <th>isItFullTime</th>\n",
    "    <th>description</th>\n",
    "    <th>startDate</th>\n",
    "    <th>fees</th>\n",
    "    <th>modality</th>\n",
    "    <th>duration</th>\n",
    "    <th>city</th>\n",
    "    <th>country</th>\n",
    "    <th>administration</th>\n",
    "    <th>url</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td>0</td>\n",
    "    <td> Accounting and Finance - MSc</td>\n",
    "    <td>University of Leeds</td>\n",
    "    <td>Leeds University Business School</td>\n",
    "    <td>Full time</td>\n",
    "    <td>Businesses and governments rely on [...].</td>\n",
    "    <td>September</td>\n",
    "    <td>UK: £18,000 (Total) International: £34,750 (Total)</td>\n",
    "    <td>MSc</td>\n",
    "    <td>1 year full time</td>\n",
    "    <td>Leeds</td>\n",
    "    <td>United Kingdom</td>\n",
    "    <td>On Campus</td>\n",
    "    <td><a href=\"https://www.findamasters.com/masters-degrees/course/accounting-and-finance-msc/?i321d3232c3891\">Link</a></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td> Accounting, Accountability & Financial Management MSc</td>\n",
    "    <td>King’s College London</td>\n",
    "    <td>King’s Business School</td>\n",
    "    <td>Full time</td>\n",
    "    <td>Our Accounting, Accountability & Financial Management MSc course will provide [...].</td>\n",
    "    <td>September</td>\n",
    "    <td>Please see the university website for further information on fees for this course.</td>\n",
    "    <td>MSc</td>\n",
    "    <td>1 year FT</td>\n",
    "    <td>London</td>\n",
    "    <td>United Kingdom</td>\n",
    "    <td>On Campus</td>\n",
    "    <td><a href=\"https://www.findamasters.com/masters-degrees/course/accounting-accountability-and-financial-management-msc/?i132d7816c25522\">Link</a></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td> Accounting, Financial Management and Digital Business - MSc</td>\n",
    "    <td>University of Reading</td>\n",
    "    <td>Henley Business School</td>\n",
    "    <td>Full time</td>\n",
    "    <td>Embark on a professional accounting career [...].</td>\n",
    "    <td>September</td>\n",
    "    <td>Please see the university website for further information on fees for this course.</td>\n",
    "    <td>MSc</td>\n",
    "    <td>1 year full time</td>\n",
    "    <td>Reading</td>\n",
    "    <td>United Kingdom</td>\n",
    "    <td>On Campus</td>\n",
    "    <td><a href=\"https://www.findamasters.com/masters-degrees/course/accounting-financial-management-and-digital-business-msc/?i345d4286c351\">Link</a></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td> Addictions MSc</td>\n",
    "    <td>King’s College London</td>\n",
    "    <td>Institute of Psychiatry, Psychology and Neuroscience</td>\n",
    "    <td>Full time</td>\n",
    "    <td>Join us for an online session for prospective [...].</td>\n",
    "    <td>September</td>\n",
    "    <td>Please see the university website for further information on fees for this course.</td>\n",
    "    <td>MSc</td>\n",
    "    <td>One year FT</td>\n",
    "    <td>London</td>\n",
    "    <td>United Kingdom</td>\n",
    "    <td>On Campus</td>\n",
    "    <td><a href=\"https://www.findamasters.com/masters-degrees/course/addictions-msc/?i132d4318c27100\">Link</a></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>4</td>\n",
    "    <td> Advanced Chemical Engineering - MSc</td>\n",
    "    <td>University of Leeds</td>\n",
    "    <td>School of Chemical and Process Engineering</td>\n",
    "    <td>Full time</td>\n",
    "    <td>The Advanced Chemical Engineering MSc at Leeds [...].</td>\n",
    "    <td>September</td>\n",
    "    <td>UK: £13,750 (Total) International: £31,000 (Total)</td>\n",
    "    <td>MSc</td>\n",
    "    <td>1 year full time</td>\n",
    "    <td>Leeds</td>\n",
    "    <td>United Kingdom</td>\n",
    "    <td>On Campus</td>\n",
    "  </tr>\n",
    "  <!-- Add more rows here as needed -->\n",
    "</tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "\n",
    "For each master's degree, you create a `course_i.tsv` file of this structure:\n",
    "\n",
    "```\n",
    "courseName \\t universityName \\t  ... \\t url\n",
    "```\n",
    "\n",
    "If an information is missing, you just leave it as an empty string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inizialize dataframe\n",
    "columns = ['courseName', 'universityName','facultyName','isItFullTime','description','startDate','fees','modality','duration','city','country','adminsitration','url']\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d89ccbaff940829493074e3a9f9750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating tsv files:   0%|          | 0/400 [00:00<?, ?URL/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For all the folders\n",
    "for number in tqdm(range(1,401), desc= \"Creating tsv files\", unit=\"URL\"):\n",
    "\n",
    "    # Path of folder\n",
    "    folder_path = f'/Users/Francesco/Desktop/ADM/HW3/Page{number}'\n",
    "\n",
    "    # List of files in directory\n",
    "    file_list = os.listdir(folder_path)\n",
    "    file_list = [f for f in file_list if f.endswith('.html')] # On Mac there is a file called .DS_store, we don't want it\n",
    "\n",
    "    # For each file\n",
    "    for i,file_name in enumerate(file_list):\n",
    "        \n",
    "        # Full path of file (folder path + file path)\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        # Read file\n",
    "        with open(file_path, 'rb') as file:\n",
    "            file_content = file.read()\n",
    "\n",
    "        # Get infos of the page into a list\n",
    "        content = crawler.extract_MSc_content(file_content)\n",
    "\n",
    "        # Create tsv file using name of the course\n",
    "        file_name = f'course_{i}'\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        with open(f'{file_path}.tsv','w') as tsvfile:\n",
    "            tsvfile.write(f'{content[0]} \\t {content[1]} \\t {content[2]} \\t {content[3]} \\t {content[4]} \\t {content[5]} \\t {content[6]} \\t {content[7]} \\t {content[8]} \\t {content[9]} \\t {content[10]} \\t {content[11]} \\t {content[12]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the .tsv files into one df\n",
    "for number in tqdm(range(1,401), desc= \"Merging tsv files\", unit=\"URL\"):\n",
    "\n",
    "    # Path of folder\n",
    "    folder_path = f'/Users/Francesco/Desktop/ADM/HW3/Page{number}'\n",
    "\n",
    "    # List of files in directory\n",
    "    file_list = os.listdir(folder_path)\n",
    "    file_list = [f for f in file_list if f.endswith('.tsv')] # On Mac there is a file called .DS_store, we don't want it\n",
    "\n",
    "    # For each file\n",
    "    for file_name in file_list:\n",
    "\n",
    "        # Full path of file\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        # Read the .tsv file\n",
    "        df = pd.read_csv(file_path, sep='\\t',names=columns)\n",
    "\n",
    "        # Append df into a list\n",
    "        data.append(df)\n",
    "\n",
    "# Concat all the df into one       \n",
    "merged_df = pd.concat(data, ignore_index=True)\n",
    "\n",
    "# Download df\n",
    "merged_df.to_csv('/Users/Francesco/Desktop/ADM/HW3/df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Search Engine\n",
    "\n",
    "Now, we want to create two different Search Engines that, given as input a query, return the courses that match the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Preprocessing \n",
    "\n",
    "### 2.0.0)  Preprocessing the text\n",
    "\n",
    "First, you must pre-process all the information collected for each MSc by:\n",
    "\n",
    "1. Removing stopwords\n",
    "2. Removing punctuation\n",
    "3. Stemming\n",
    "4. Anything else you think it's needed\n",
    "   \n",
    "For this purpose, you can use the [`nltk library](https://www.nltk.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0.1) Preprocessing the fees column\n",
    "\n",
    "Moreover, we want the field ```fees``` to collect numeric information. As you will see, you scraped textual information for this attribute in the dataset: sketch whatever method you need (using regex, for example, to find currency symbol) to collect information and, in case of multiple information, retrieve only the highest fees. Finally, once you have collected numerical information, you likely will have different currencies: this can be chaotic, so let chatGPT guide you in the choice and deployment of an API to convert this column to a common currency of your choice (it can be USD, EUR or whatever you want). Ultimately, you will have a ```float``` column renamed ```fees (CHOSEN COMMON CURRENCY)```.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Conjunctive query\n",
    "For the first version of the search engine, we narrowed our interest to the __description__ of each course. It means that you will evaluate queries only concerning the course's description.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1) Create your index!\n",
    "\n",
    "Before building the index, \n",
    "* Create a file named `vocabulary`, in the format you prefer, that maps each word to an integer (`term_id`).\n",
    "\n",
    "Then, the first brick of your homework is to create the Inverted Index. It will be a dictionary in this format:\n",
    "\n",
    "```\n",
    "{\n",
    "term_id_1:[document_1, document_2, document_4],\n",
    "term_id_2:[document_1, document_3, document_5, document_6],\n",
    "...}\n",
    "```\n",
    "where _document\\_i_ is the *id* of a document that contains that specific word.\n",
    "\n",
    "__Hint:__ Since you do not want to compute the inverted index every time you use the Search Engine, it is worth thinking about storing it in a separate file and loading it in memory when needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2) Execute the query\n",
    "Given a query input by the user, for example:\n",
    "\n",
    "```\n",
    "advanced knowledge\n",
    "```\n",
    "\n",
    "The Search Engine is supposed to return a list of documents.\n",
    "\n",
    "##### What documents do we want?\n",
    "Since we are dealing with conjunctive queries (AND), each returned document should contain all the words in the query.\n",
    "The final output of the query must return, if present, the following information for each of the selected documents:\n",
    "\n",
    "* `courseName`\n",
    "* `universityName`\n",
    "* `description`\n",
    "* `URL`\n",
    "\n",
    "__Example Output__ for ```advanced knowledge```: (please note that our examples are made on a small batch of the full dataset)\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"img/output1.png\" width = 1000>\n",
    "</p>\n",
    "\n",
    "If everything works well in this step, you can go to the next point and make your Search Engine more complex and better at answering queries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Conjunctive query & Ranking score\n",
    "\n",
    "For the second search engine, given a query, we want to get the *top-k* (the choice of *k* it's up to you!) documents related to the query. In particular:\n",
    "\n",
    "* Find all the documents that contain all the words in the query.\n",
    "* Sort them by their similarity with the query.\n",
    "* Return in output *k* documents, or all the documents with non-zero similarity with the query when the results are less than _k_. You __must__ use a heap data structure (you can use Python libraries) for maintaining the *top-k* documents.\n",
    "\n",
    "To solve this task, you must use the *tfIdf* score and the _Cosine similarity_. The field to consider is still the `description`. Let's see how.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1) Inverted index\n",
    "Your second Inverted Index must be of this format:\n",
    "\n",
    "```\n",
    "{\n",
    "term_id_1:[(document1, tfIdf_{term,document1}), (document2, tfIdf_{term,document2}), (document4, tfIdf_{term,document4}), ...],\n",
    "term_id_2:[(document1, tfIdf_{term,document1}), (document3, tfIdf_{term,document3}), (document5, tfIdf_{term,document5}), (document6, tfIdf_{term,document6}), ...],\n",
    "...}\n",
    "```\n",
    "\n",
    "Practically, for each word, you want the list of documents in which it is contained and the relative *tfIdf* score.\n",
    "\n",
    "__Tip__: *TfIdf* values are invariant for the query. Due to this reason, you can precalculate and store them accordingly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2) Execute the query\n",
    "\n",
    "In this new setting, given a query, you get the proper documents (i.e., those containing all the query's words) and sort them according to their similarity to the query. For this purpose, as the scoring function, we will use the Cosine Similarity concerning the *tfIdf* representations of the documents.\n",
    "\n",
    "Given a query input by the user, for example:\n",
    "```\n",
    "advanced knowledge\n",
    "```\n",
    "The search engine is supposed to return a list of documents, __ranked__ by their Cosine Similarity to the query entered in the input.\n",
    "\n",
    "More precisely, the output must contain:\n",
    "* `courseName`\n",
    "* `universityName`\n",
    "* `description`\n",
    "* `URL`\n",
    "* The similarity score of the documents with respect to the query (float value between 0 and 1)\n",
    "  \n",
    "__Example Output__ for ```advanced knowledge```:\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"img/output2.png\" width = 1000>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define a new score!\n",
    "\n",
    "Now it's your turn: build a new metric to rank MSc degrees.\n",
    "\n",
    "Practically:\n",
    "\n",
    "1. The user will enter a text query. As a starting point, get the query-related documents by exploiting the search engine of Step 2.1.\n",
    "2. Once you have the documents, you need to sort them according to your new score. In this step, you won't have any more to take into account just the ```description``` field of the documents; you __can__ use also the remaining variables in your dataset (or new possible variables that you can create from the existing ones or scrape again from the original web-pages). You __must__ use a heap data structure (you can use Python libraries) for maintaining the *top-k* documents.\n",
    "\n",
    "__N.B.:__ You have to define a __scoring function__, not a filter! \n",
    "\n",
    "The output, must contain:\n",
    "\n",
    "* `courseName`\n",
    "* `universityName`\n",
    "* `description`\n",
    "* `URL`\n",
    "* The  __new__ similarity score of the documents with respect to the query\n",
    "\n",
    "Are the results you obtain better than with the previous scoring function? **Explain and compare results**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizing the most relevant MSc degrees\n",
    "\n",
    "Using maps can help people understand how far one university is from another so they can plan their academic careers more adequately. Here, we challenge you to show a map of the courses found with the score defined in point 3. You should be able to identify at least the *city* and *country* for each MSc degree. You can find some ideas on how to create maps in Python [here](https://plotly.com/python/maps/) and [here](https://towardsdatascience.com/visualizing-geospatial-data-in-python-e070374fe621) but you will maybe need further information for a proper visualization, like coordinates (latitude and longitude). You can retrieve this data using various tools:\n",
    "\n",
    "1. [Here](https://medium.com/@manilwagle/geocoding-the-world-using-google-api-and-python-1f6b6fb6ca48) you can find a helpful tutorial on how to encode geo-informations using Google API in Python (this tool can also be used in [Google Sheets](https://handsondataviz.org/geocode.html))\n",
    "2. You can collect a list of unique places in the format (City, Country) and ask chatGPT (or, as usual, any other LLM chatbot) to provide you with a list of corresponding representative coordinates\n",
    "3. Explore and find the best solution for your case!\n",
    "   \n",
    "Once you defined your visualization strategy, include a way to encode fees in your charts. The map should show (with a proper legend) different courses and associated taxation: the user wants a glimpse not only of how far he will need to move but also of how much it will cost him!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. BONUS: More complex search engine \n",
    "\n",
    "__IMPORTANT:__ This is a bonus step, so it's <ins>not mandatory</ins>. You can get the maximum score also without doing this. We will take this into account, **only if** the rest of the homework has been completed.\n",
    "\n",
    "For the Bonus part, we want to ask you more sophisticated search engine. Here we want to let users issue more complex queries. The options of this new search engine are: \n",
    "1. Give the possibility to specify queries for the following features (the user should have the option to issue __none or all of them__): \n",
    "* `courseName`\n",
    "* `universityName`\n",
    "* `universityCity`\n",
    "2. Specify a range for the __fees__ to retrieve only MSc whose taxation is in that range.\n",
    "3. Specify a list of __countries__ which the search engine should only return the courses taking place in city within those countries.\n",
    "4. Filter based on the courses that have already started. \n",
    "5. Filter based on the presence of online modality. \n",
    "\n",
    "__Note 1__: You should be aware that you should give the user the possibility <ins>to select any</ins> of the abovementioned options. How should the user use the options? We will accept __any manual__ that you provide to the user. \n",
    "\n",
    "__Note 2__: As you may have realized from __1st option__, you need to build <ins>inverted indexes</ins> for those values and return all of the documents that have the similarity <ins>more than 0</ins> concerning the given queries. Choose a __logical__ way to aggregate the similarity coming from each of them and explain your idea in detail.\n",
    "\n",
    "__Note 3__: The options <ins>other than 1st</ins> one can be considered as __filtering criteria__ so the retrieved documents <ins>must respect all</ins> of those filters. \n",
    "\n",
    "The output must contain the following information about the places:\n",
    "\n",
    "* `courseName`\n",
    "* `universityName`\n",
    "* `url`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Command Line Question\n",
    "\n",
    "As done in the previous assignment, we encourage using the command as a feature that Data Scientists must master.\n",
    "\n",
    "Note: To answer the question in this section, you must strictly use command line tools. We will reject any other method of response. The final script must be placed in CommandLine.sh.\n",
    "\n",
    "First, take the course_i.tsv files you created in point 1 and merge them using Linux commands (Hint: make sure that the first row containing the column names appears only once).\n",
    "\n",
    "Now that you have your merged file named merged_courses.tsv, use Linux commands to answer the following questions:\n",
    "- Which country offers the most Master's Degrees? Which city?\n",
    "- How many colleges offer Part-Time education?\n",
    "- Print the percentage of courses in Engineering (the word \"Engineer\" is contained in the course's name).\n",
    "\n",
    "__Important note:__ You may work on this question in any environment (AWS, your PC command line, Jupyter notebook, etc.), but the final script must be placed in CommandLine.sh, which must be executable. Please run the script and include a __screenshot__ of the <ins>output</ins> in the notebook for evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Algorithmic Question \n",
    "Leonardo is an intern at a company. He is paid based on the total number of hours he has worked. They agreed __d__ days ago that Leonardo could not work less than $minTime_i$ or more than $maxTime_i$ hours per <ins>i-th</ins> day. Furthermore, he was warned by HR that on his last day at the company, he should provide a detailed report on how many hours he worked <ins>each day</ins> for the previous d days.\n",
    "\n",
    "Today is the day Leonardo should report to HR, but the problem is that he <ins>didn't</ins> account for how many hours he put in for each day, so he only has the __total sum of the hours__ ($sumHours$) he put in total in these d days. He believes that if he creates a report in which each number $dayHours_i$ corresponds to the __total hours he worked on the i-th day__ while satisfying the HR limitations and the total sum of all $dayHours_i$ equals $sumHours$, he would be fine.\n",
    "\n",
    "He cannot create such a report independently and requests your assistance. He will give you the number of days $d$, total hours spent $sumHours$, and the HR limitations for each day $i$, and he wants you to assist him in determining whether it is possible to create such a fake report. If that is possible, make such a report. \n",
    "\n",
    "**Input**\n",
    "\n",
    "The first line of input contains two integers __d__, $sumHours$ - the number of days Leonardo worked there and the total number of hours he worked for the company. Each of the following __d__ lines contains two integer numbers $minTime_i$ and $maxTime_i$ - the minimum and maximum hours he can work on the $i_{th}$ day. \n",
    "\n",
    "**Output**\n",
    "\n",
    "If such a report cannot be generated, print 'NO' in one output line. If such a report is possible, print 'YES' in the output and d numbers - the number of hours Leonardo spent each day - in the second line. If more than one solution exists, print any of them. \n",
    "\n",
    "__Input 1__\n",
    "```\n",
    "2 5\n",
    "0 1\n",
    "3 5\n",
    "```\n",
    "__Output 1__\n",
    "```\n",
    "YES\n",
    "1 4 \n",
    "```\n",
    "---\n",
    "__Input 2__\n",
    "```\n",
    "1 1\n",
    "5 6\n",
    "```\n",
    "__Output 2__\n",
    "```\n",
    "NO\n",
    "```\n",
    "\n",
    "1. Implement a code to solve the above mentioned problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report():\n",
    "    # 'value'    --> is a list that contains the total number of days worked and the total number of hours worked \n",
    "    # 'remaning' --> is a list used to store the condition 'YES' or 'NO' and \n",
    "    #                the remaining hours of work that are not assign to a specific day\n",
    "    # 'i'        --> is an index\n",
    "    value, remaning, i = list(map(int, input().split())), [\"\", 0], 0\n",
    "    \n",
    "    # We create a list of lists composed of a set of values containing the minimum hours worked, \n",
    "    # the maximum hours worked and the hours actually worked initialized to the minimum value\n",
    "    # Every item in this list of list is a different day\n",
    "    days = [[minimo, massimo, minimo] for _ in range(value[0]) for minimo, massimo in [map(int, input().split())]]\n",
    "    \n",
    "    # Check if the sum of maximum possible working hours for all days is greater or equal to \n",
    "    # the total number of hours worked and if the sum of minumum working hours is lower than \n",
    "    # the total of hours worked\n",
    "    # --> if the condition is TRUE than we can generate a report with the desired features \n",
    "    #     --> we set the values of 'remaning' equal to 'YES' and equal to the the remaining hours of work \n",
    "    #         that are not assigned to a specific day\n",
    "    # --> if the condition is FALSE than we can not generate a report with the desired features\n",
    "    #     --> we set the values of 'remaning' equal to 'NO' and 0 \n",
    "    remaning = [\"YES\", (value[1] - sum(item[0] for item in days))] if (sum(item[1] for item in days) >= value[1]) & (sum(item[1] for item in days) <= value[1]) else [\"NO\", 0] \n",
    "    \n",
    "    # If we can not generate a report with the desired features then we return 'NO'\n",
    "    if remaning[0] == \"NO\": return remaning[0]\n",
    "    \n",
    "    # If we get to this point it is implied that we can generate a report with the desired features\n",
    "    # As long as the number of hours that are not assigned is greater than 0 it means that in some days \n",
    "    #    we have worked more than the minimum number of hours\n",
    "    while remaning[1] > 0:\n",
    "        # Update the working hours for the current day to the maximum\n",
    "        # We reduce the number of hours that we have not yet assigned \n",
    "        #    by the number of hours we have just added to the i-th day (maximum-minimum)\n",
    "        # We add 1 to the index\n",
    "        days[i][2], remaning[1], i = days[i][1], remaning[1] - (days[i][1] - days[i][0]), i + 1\n",
    "    \n",
    "    # If there is a negative remaining of hours that we have not yet assigned \n",
    "    #    it means that we added too many hours to the last day set to the maximum hours \n",
    "    # So we remove this excess from the last day\n",
    "    # --> we use the + operator because the remaining number is negative\n",
    "    if remaning[1] < 0:\n",
    "        days[i - 1][2] = days[i - 1][2] + remaning[1]\n",
    "    \n",
    "    # Return the result in the specified format\n",
    "    return f'{remaning[0]}\\n{\" \".join(map(str, list(zip(*days))[2]))}'\n",
    "\n",
    "# Print the result of the report-function\n",
    "print(report())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What is the __time complexity__ (the Big O notation) of your solution? Please provide a <ins>detailed explanation</ins> of how you calculated the time complexity.\n",
    "\n",
    "3. Ask ChatGPT or any other LLM chatbot tool to check your code's time complexity (the Big O notation). Compare your answer to theirs. Do you believe this is correct? If the <ins>two differ</ins>, which one is right? (why?)\n",
    "   \n",
    "4. What do you think of the __optimality__ of your code? Do you believe it is optimal? Can you improve? Please <ins>elaborate</ins> on your response. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
